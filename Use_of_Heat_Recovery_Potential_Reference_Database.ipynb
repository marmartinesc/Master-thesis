{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9b920b-c864-4756-a9df-3ff24858e665",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 as db\n",
    "import pandas as pd\n",
    "import re\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "\n",
    "# List of common suffixes to remove\n",
    "suffixes = ['gmbh', 'co', 'kg', 'inc', 'llc', 'ltd', 'ag', 'corporation', 'corp','deutschland','raffinerie','oel','werk','nord','sud','europa','holding','europe','se','oil','aluminium']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab40c79c-b215-4460-966b-746b8a6de344",
   "metadata": {},
   "source": [
    "## 1. Load databases and extract key words from company names for further comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c784ac56-d32d-405c-896d-ea92b031bb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load heat production database with the geolocated addresses to add a column for the Postal Code\n",
    "conn = db.connect('database.db')\n",
    "df_hpp = pd.read_sql_query('select * from heatpotentialpostalcode', conn)\n",
    "conn.close()\n",
    "\n",
    "df_hpp.rename(columns={'zip': 'PostalCode'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4baafbbc-5b69-4bb6-8129-c66eb44b2688",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_hpp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65e32b6-3135-4e0e-a363-33868eab1556",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleanednamehpp= df_hpp\n",
    "\n",
    "# Function to clean company names\n",
    "def clean_name(name):\n",
    "    # Remove common suffixes and extra whitespace\n",
    "    pattern = r'\\b(?:' + '|'.join(suffixes) + r')\\b'\n",
    "    # Remove special characters (e.g., & . ,)\n",
    "    name = re.sub(r'[&.,+()]', '', name)\n",
    "    \n",
    "    return re.sub(pattern, '', name, flags=re.IGNORECASE).strip()\n",
    "\n",
    "# Apply the clean_name function to create a new column for comparison\n",
    "df_cleanednamehpp['CleanedName'] = df_cleanednamehpp['CompanyName'].apply(clean_name)\n",
    "\n",
    "print(\"length df: \", len(df_cleanednamehpp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320a6395-4614-4cd0-b797-1123464d5307",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load handelsregister\n",
    "conn = db.connect('latlongsdata.db')\n",
    "df_h = pd.read_sql_query('select * from Lat_Long_Table_HandelsregisterV3', conn)\n",
    "#df_h = pd.read_sql_query('select * from Lat_Long_Table_Handelsregister_Referencepaper', conn)\n",
    "conn.close()\n",
    "df_h = df_h.drop_duplicates(subset=['name','register_identifier','zip'], keep='first')\n",
    "print(len(df_h))\n",
    "#df_h = df_h.drop(columns=['level_0','index'])\n",
    "df_h.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c46c55-3e6d-43ab-a247-599042a58df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleanednameh = df_h\n",
    "\n",
    "# Function to clean company names\n",
    "def clean_name(name):\n",
    "    # Remove common suffixes and extra whitespace\n",
    "    pattern = r'\\b(?:' + '|'.join(suffixes) + r')\\b'\n",
    "    # Remove special characters (e.g., & . ,)\n",
    "    name = re.sub(r'[&.,+()]', '', name)\n",
    "    \n",
    "    return re.sub(pattern, '', name, flags=re.IGNORECASE).strip()\n",
    "\n",
    "# Apply the clean_name function to create a new column for comparison\n",
    "df_cleanednameh['CleanedName'] = df_cleanednameh['name'].apply(clean_name)\n",
    "\n",
    "print(\"length df: \", len(df_cleanednameh))\n",
    "df_cleanednameh.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3392dd5c-4897-4c69-a533-706024fdf1bc",
   "metadata": {},
   "source": [
    "## 2. Finding coincidences based on comparing company names' key words and then postal codes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd77df6-1a3b-4e53-80ab-7927f95bcd2f",
   "metadata": {},
   "source": [
    "### A. Compare names and then postal codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7256e91-0d91-4717-8b22-3cb4e7313f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_cleanednamehpp\n",
    "\n",
    "df2 = df_cleanednameh\n",
    "\n",
    "# Create an empty list to store merged rows\n",
    "merged_rows = []\n",
    "\n",
    "# Iterate through each row in df1\n",
    "for i, row1 in df1.iterrows():\n",
    "    words1 = set(row1['CleanedName'].split())\n",
    "    \n",
    "    # Compare with each row in df2\n",
    "    for j, row2 in df2.iterrows():\n",
    "        words2 = set(row2['CleanedName'].split())\n",
    "        \n",
    "        # If there's at least one common word, merge the rows\n",
    "        if words1.intersection(words2):\n",
    "            merged_row = {**row1, **row2}  # Merge the two rows into one dictionary\n",
    "            merged_rows.append(merged_row)  # Add the merged row to the list\n",
    "\n",
    "# Convert the list of merged rows into a new DataFrame\n",
    "merged_df = pd.DataFrame(merged_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e3bba0-7160-4d59-9be3-8eb6a45cb6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep rows where Postalcodes coincide \n",
    "filtered_df = merged_df[merged_df['zip'] == merged_df['PostalCode']]\n",
    "filtered_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9572154-0b30-45d9-ab2e-9978fdd8c61c",
   "metadata": {},
   "source": [
    "### B. Comparing postal codes and then names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7893486-f53f-4582-8d3a-cd2b58c52c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_hpp\n",
    "\n",
    "df2 = df_h\n",
    "\n",
    "# Merge the two dataframes on the Postal code column\n",
    "merged_df = pd.merge(df1, df2, left_on='PostalCode', right_on='zip', how='inner')\n",
    "print(len(merged_df))\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9f5eae-c320-4fcc-a7c2-cd6f38f07a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean and split text into words\n",
    "def clean_and_split(text):\n",
    "    # Remove special characters and split into words\n",
    "    words = re.sub(r'[^\\w\\s]', '', text).lower().split()\n",
    "    return [word for word in words if word not in suffixes]\n",
    "\n",
    "# Apply the function to both columns\n",
    "merged_df['name_words'] = merged_df['name'].apply(clean_and_split)\n",
    "merged_df['CompanyName_words'] = merged_df['CompanyName'].apply(clean_and_split)\n",
    "\n",
    "# Function to check if there is any common word between two lists\n",
    "def has_common_word(list1, list2):\n",
    "    return any(word in list2 for word in list1)\n",
    "\n",
    "# Filter the DataFrame\n",
    "filteredmerged_df = merged_df[merged_df.apply(lambda row: has_common_word(row['name_words'], row['CompanyName_words']), axis=1)]\n",
    "\n",
    "# Drop the helper columns if needed\n",
    "filteredmerged_df = filteredmerged_df.drop(columns=['name_words', 'CompanyName_words'])\n",
    "filteredmerged_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367e5ee8-7012-46a4-ad18-44d037664c6c",
   "metadata": {},
   "source": [
    "## C. Now we merge both (1. filtered_df and 2. filteredmerged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13727a3-97ca-466c-8752-5d79aef1b2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the two DataFrames\n",
    "merged_dfheatpot = pd.concat([filteredmerged_df, filtered_df])\n",
    "\n",
    "# Drop duplicate rows\n",
    "# Optionally, you can specify which columns to consider for detecting duplicates\n",
    "merged_df.drop_duplicates(subset=['level_1_Tj', 'CompanyName'], keep='first')\n",
    "merged_dfheatpot = merged_dfheatpot.drop_duplicates()\n",
    "len(merged_dfheatpot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13764398-71a7-4df2-b782-dd1f380d92d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dfheatpot = merged_dfheatpot.drop(columns=['StreetNameAndNumber','Country','CleanedName_x','CleanedName_y','zip'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30a913d-d7e0-4800-a7bc-34d97b5cbfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dfheatpot = merged_dfheatpot.drop(columns=['CleanedName'])\n",
    "merged_dfheatpot = merged_dfheatpot.drop_duplicates()\n",
    "len(merged_dfheatpot)\n",
    "finaldf = merged_dfheatpot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33eef396-cd6c-4bdb-9400-4a85b407e72c",
   "metadata": {},
   "source": [
    "## 3. Check from which documents I can get information from the XML files available in the Handelsregister"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595b0aaf-6162-45d0-a8a4-9e087ee0dcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#antes de runear esto asegurarme de que todos los si docs nuevos est√°n en la carpeta de si docs from other tries \n",
    "import os\n",
    "\n",
    "df = merged_dfheatpot\n",
    "df = df[['name','register_identifier']]\n",
    "\n",
    "df = df.rename(columns={'name': 'Name', 'register_identifier': 'Reference_number'})\n",
    "\n",
    "# Directory where the documents are stored\n",
    "doc_directory = r\"Directory_X\"\n",
    "\n",
    "# Lists to store company names and reference numbers with and without matching documents\n",
    "matching_companies = []\n",
    "matching_refnum = []\n",
    "no_matching_companies = []\n",
    "no_matching_refnum = []\n",
    "\n",
    "# Iterate through each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    company_name = row['Name']\n",
    "    reference_number = str(row['Reference_number'])\n",
    "\n",
    "    # Flag to check if a matching document was found\n",
    "    found = False\n",
    "\n",
    "    # Search term is simply the reference number\n",
    "    search_term = reference_number\n",
    "\n",
    "    for doc_name in os.listdir(doc_directory):\n",
    "        if search_term in doc_name:\n",
    "            matching_companies.append(company_name)\n",
    "            matching_refnum.append(reference_number)\n",
    "            found = True\n",
    "            break  # Exit loop if a match is found\n",
    "\n",
    "    if not found:\n",
    "        #print(f\"No document found for {company_name} with reference number {reference_number}\")\n",
    "        no_matching_companies.append(company_name)\n",
    "        no_matching_refnum.append(reference_number)\n",
    "\n",
    "\n",
    "print(\"Xml files found for \", len(matching_companies), \"companies\")\n",
    "print(\"Xml files not found for \", len(no_matching_companies), \"companies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1a9c4c-607e-478c-9d2c-01f8a12adbb9",
   "metadata": {},
   "source": [
    "## 4. Extract GRUNDKAPITAL / STAMMKAPITAL / HAFTEINLAGE values and add them to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efd2749-0aac-477f-bc36-c16a22659591",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as et\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming matching_companies and matching_refnum are already defined\n",
    "combined_data = list(zip(matching_companies, matching_refnum))\n",
    "\n",
    "# Create DataFrame from combined data\n",
    "df_grundkapital = pd.DataFrame(combined_data, columns=['Name', 'Reference_number'])\n",
    "stammkapital = []\n",
    "grundkapital = []\n",
    "hafteinlage = []\n",
    "\n",
    "# Directory containing XML files\n",
    "download_directory = r\"Directory_X\"\n",
    "\n",
    "# Define the namespaces\n",
    "namespaces = {\n",
    "    'xjustiz': 'http://www.xjustiz.de'\n",
    "}\n",
    "\n",
    "# Iterate over each company\n",
    "for company in df_grundkapital.Name:\n",
    "    # Get the reference number of the company\n",
    "    reference_number = str(df_grundkapital[df_grundkapital['Name'] == company]['Reference_number'].iloc[0])\n",
    "    \n",
    "    xml_file_path = None\n",
    "    for xml_file in os.listdir(download_directory):\n",
    "        if xml_file.endswith(\".xml\") and reference_number in xml_file:\n",
    "            xml_file_path = os.path.join(download_directory, xml_file)\n",
    "            break  # Exit loop if a matching file is found\n",
    "\n",
    "    if xml_file_path:\n",
    "        with open(xml_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            xml_content = file.read()\n",
    "        \n",
    "        xml_tree = et.fromstring(xml_content)\n",
    "\n",
    "        # STAMMKAPITAL\n",
    "        stammkapital_value = xml_tree.findall(\".//xjustiz:fachdatenRegister/xjustiz:auswahl_zusatzangaben/xjustiz:kapitalgesellschaft/xjustiz:zusatzGmbH/xjustiz:stammkapital/xjustiz:zahl\", namespaces)\n",
    "        stammkapital.append(stammkapital_value[0].text if stammkapital_value else \"0\")\n",
    "\n",
    "        # GRUNDKAPITAL\n",
    "        grundkapital_value = xml_tree.findall(\".//xjustiz:fachdatenRegister/xjustiz:auswahl_zusatzangaben/xjustiz:kapitalgesellschaft/xjustiz:zusatzAktiengesellschaft/xjustiz:grundkapital/xjustiz:hoehe/xjustiz:zahl\", namespaces)\n",
    "        grundkapital.append(grundkapital_value[0].text if grundkapital_value else \"0\")\n",
    "\n",
    "        # HAFTEINLAGE - Extract all, sum them up, and append to the list\n",
    "        hafteinlage_elements = xml_tree.findall(\".//xjustiz:fachdatenRegister/xjustiz:auswahl_zusatzangaben/xjustiz:personengesellschaft/xjustiz:zusatzGmbH/xjustiz:datenKommanditist/xjustiz:hafteinlage/xjustiz:zahl\", namespaces)\n",
    "        if not hafteinlage_elements:\n",
    "            hafteinlage_elements = xml_tree.findall(\".//xjustiz:fachdatenRegister/xjustiz:auswahl_zusatzangaben/xjustiz:personengesellschaft/xjustiz:zusatzKG/xjustiz:datenKommanditist/xjustiz:hafteinlage/xjustiz:zahl\", namespaces)\n",
    "        \n",
    "        total_hafteinlage = sum(float(el.text) for el in hafteinlage_elements)\n",
    "        hafteinlage.append(str(total_hafteinlage) if hafteinlage_elements else \"0\")\n",
    "    else:\n",
    "        stammkapital.append(\"0\")\n",
    "        grundkapital.append(\"0\")\n",
    "        hafteinlage.append(\"0\")\n",
    "\n",
    "# Add the extracted data to the DataFrame\n",
    "df_grundkapital['Stammkapital'] = stammkapital\n",
    "df_grundkapital['Grundkapital'] = grundkapital\n",
    "df_grundkapital['Hafteinlage'] = hafteinlage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a339302b-f001-4428-8cf6-eae836f91bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grundkapitaldd = df_grundkapital.drop_duplicates()\n",
    "len(df_grundkapitaldd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877fac06-7638-49f9-99bc-0dadf171e3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grundkapital.rename(columns={'Reference_number': 'register_identifier'}, inplace=True)\n",
    "combined_df = pd.merge(df_grundkapital, finaldf, on='register_identifier', how='inner')\n",
    "combined_df = combined_df.drop(columns=['Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9ca6b7-7042-4502-9c16-568159e7a384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to SQLite database (or create it if it doesn't exist)\n",
    "conn = db.connect('database.db')\n",
    "\n",
    "\n",
    "# Save the DataFrame to a table in the SQLite database\n",
    "combined_df.to_sql('HeatPotential-Capital', conn, if_exists='replace', index=False)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40c68a1-87fc-4a6f-95a3-5590bb834283",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(combined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93c01d8-8894-42f7-86bd-023d68a458e0",
   "metadata": {},
   "source": [
    "## 5. Save names for which results were not found for further searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa197ead-a32f-4876-a491-0ae44e55e473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame with rows from df_ref where CompanyName does NOT appear in df_1234\n",
    "filtered_df_hpp = df_hpp[~df_hpp['CompanyName'].isin(combined_df['CompanyName'])]\n",
    "\n",
    "filtered_df_hpp = filtered_df_hpp.drop(columns=['zip','Country','Eurostat_Name','level_1_Tj', 'level_2_Tj', 'level_3_Tj', 'level_1_r_Tj','level_2_r_Tj','level_3_r_Tj'])\n",
    "\n",
    "filtered_df_hpp = filtered_df_hpp.assign(source='Heat pot')\n",
    "filtered_df_hpp.rename(columns={'StreetNameAndNumber': 'Address'}, inplace=True)\n",
    "\n",
    "conn = db.connect('database.db')\n",
    "\n",
    "# Save the DataFrame to a table in the SQLite database\n",
    "filtered_df_hpp.to_sql('NotMatched', conn, if_exists='append', index=False)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n",
    "\n",
    "filtered_df_hpp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988238db-849c-4847-80f1-06606020f302",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = list(zip(no_matching_companies, no_matching_refnum))\n",
    "combined_data = pd.DataFrame(combined_data, columns=['Name', 'Reference_number'])\n",
    "\n",
    "\n",
    "conn = db.connect('database.db')\n",
    "\n",
    "# Save the DataFrame to a table in the SQLite database\n",
    "combined_data.to_sql('NotMatched', conn, if_exists='append', index=False)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d01e5e7-d604-4015-81c4-ac24790b730b",
   "metadata": {},
   "source": [
    "## 6. Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca42b94a-f498-4e5d-a901-34cbc6790325",
   "metadata": {},
   "outputs": [],
   "source": [
    "df =combined_df\n",
    "# Condition to check if all relevant columns have 0\n",
    "condition = (df[['level_1_Tj', 'level_2_Tj', 'level_3_Tj', 'level_1_r_Tj', 'level_2_r_Tj', 'level_3_r_Tj']] == 0).all(axis=1)\n",
    "\n",
    "# Filter the DataFrame to keep rows where the condition is False\n",
    "df_filtered = df[~condition]\n",
    "\n",
    "df_h2b = df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722c6f19-8af9-49d6-a2a5-b3859cacf44a",
   "metadata": {},
   "source": [
    "#### List of company names to delete: these have been found by manually checking the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0c0053-e716-4d21-a1cc-9796bbefd24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_h2b\n",
    "\n",
    "# Define the condition for the rows to be deleted\n",
    "condition1 = (df['name'] == 'comapnyname') & (df['CompanyName'] == 'companyname')\n",
    "# Delete the rows that meet the condition\n",
    "df = df[~condition1]\n",
    "\n",
    "\n",
    "\n",
    "df_h2b = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdbf075-266e-466e-b819-b55f6790af31",
   "metadata": {},
   "outputs": [],
   "source": [
    "companies_to_delete = [\n",
    "    \"company_list\"\n",
    "]\n",
    "\n",
    "# Filter the DataFrame to get the rows to delete\n",
    "deleted_rows = df_h2b[df_h2b['CompanyName'].isin(companies_to_delete)]\n",
    "\n",
    "# Filter the DataFrame to exclude the specified companies\n",
    "df_filtered = df_h2b[~df_h2b['CompanyName'].isin(companies_to_delete)]\n",
    "\n",
    "# Reset the index of the resulting DataFrame\n",
    "df_filtered = df_filtered.reset_index(drop=True)\n",
    "\n",
    "# Display the deleted rows\n",
    "#print(\"Deleted Rows:\")\n",
    "#print(deleted_rows)\n",
    "\n",
    "df_h2b = df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c013f4-8368-4548-9da8-19b020420502",
   "metadata": {},
   "source": [
    "#### Merging administrative subsidiaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461f68c4-63db-42fb-a2a7-6c91a2e3b925",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_h2b\n",
    "\n",
    "    # Groups to combine\n",
    "companies_merge = {\n",
    "    'company_kept\"=[\"company_merged\"]]\n",
    "}\n",
    "}\n",
    "\n",
    "# Columns to sum\n",
    "column_sum = ['Stammkapital', 'Grundkapital', 'Hafteinlage']\n",
    "\n",
    "# Ensure columns to sum are numeric\n",
    "df[column_sum] = df[column_sum].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Iterate through the company groups\n",
    "for company_principal, company_sec in companies_merge.items():\n",
    "    # Filter rows for the main and secondary companies\n",
    "    df_principal = df[df['name'] == company_principal]\n",
    "    df_sec = df[df['name'].isin(company_sec)]\n",
    "    \n",
    "    # If both principal and secondary companies exist\n",
    "    if not df_principal.empty and not df_sec.empty:\n",
    "        # Sum all values from secondary companies across the three columns\n",
    "        sum_val_sec = df_sec[column_sum].sum(skipna=True).sum()\n",
    "        \n",
    "        # Identify the non-zero column in the primary company\n",
    "        non_zero_col = df_principal[column_sum].iloc[0].idxmax()\n",
    "        \n",
    "        # Add the sum of the secondary companies to the non-zero column of the principal company\n",
    "        df.loc[df['name'] == company_principal, non_zero_col] += sum_val_sec\n",
    "\n",
    "        # Remove the rows of the secondary companies\n",
    "        df = df[~df['name'].isin(company_sec)]\n",
    "\n",
    "# Resulting DataFrame\n",
    "df_h2b = df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27f225a-0aba-4eeb-ba2b-4e84fd1a5aad",
   "metadata": {},
   "source": [
    "#### Grouping but distinguishing between sectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b845cd-37d0-4d73-b46f-c2717190d064",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged2 = df_h2b.groupby(['name', 'register_identifier', 'PostalCode', 'CompanyName','Eurostat_Name'], as_index=False).agg({\n",
    "    'Stammkapital': 'first',\n",
    "    'Grundkapital': 'first',\n",
    "    'Hafteinlage': 'first',\n",
    "    'location_lat': 'first',\n",
    "    'location_long': 'first',\n",
    "    'location_address': 'first',\n",
    "    'registered_address': 'first',\n",
    "    'PostalCode': 'first',\n",
    "    'Latitude': 'first',\n",
    "    'Longitude': 'first',\n",
    "    'level_1_Tj': 'sum', \n",
    "    'level_2_Tj': 'sum',  \n",
    "    'level_3_Tj': 'sum', \n",
    "    'level_1_r_Tj': 'sum',  \n",
    "    'level_2_r_Tj': 'sum',  \n",
    "    'level_3_r_Tj': 'sum',  \n",
    "    'address': 'first'\n",
    "})\n",
    "\n",
    "len(df_merged2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0cfdaa-beb4-4081-bb20-6a88e8d6219b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure columns are treated as numbers\n",
    "df = df_merged\n",
    "df['Stammkapital'] = pd.to_numeric(df['Stammkapital'])\n",
    "df['Grundkapital'] = pd.to_numeric(df['Grundkapital'])\n",
    "df['Hafteinlage'] = pd.to_numeric(df['Hafteinlage'])\n",
    "\n",
    "# Create a new column 'Capital'\n",
    "df['Capital'] = df['Stammkapital'] + df['Grundkapital'] + df['Hafteinlage']\n",
    "\n",
    "# Format 'Capital' column to have two decimal places\n",
    "df['Capital'] = df['Capital'].apply(lambda x: \"{:.2f}\".format(x))\n",
    "df_almostthere = df\n",
    "\n",
    "# Ensure columns are treated as numbers\n",
    "df = df_merged2\n",
    "df['Stammkapital'] = pd.to_numeric(df['Stammkapital'])\n",
    "df['Grundkapital'] = pd.to_numeric(df['Grundkapital'])\n",
    "df['Hafteinlage'] = pd.to_numeric(df['Hafteinlage'])\n",
    "\n",
    "# Create a new column 'Capital'\n",
    "df['Capital'] = df['Stammkapital'] + df['Grundkapital'] + df['Hafteinlage']\n",
    "\n",
    "# Format 'Capital' column to have two decimal places\n",
    "df['Capital'] = df['Capital'].apply(lambda x: \"{:.2f}\".format(x))\n",
    "df_merged3 = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59143995-75e5-4111-bdca-e4f39c32ee4f",
   "metadata": {},
   "source": [
    "##### Divide potential proportionally between the companies from Handelsregister associated to a company in the reference database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6906f5a5-e95c-4b19-88e6-ce5468836658",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Count the occurrences of each CompanyName in duplicates3\n",
    "duplicates3 = df_merged3[df_merged3.duplicated(subset=['CompanyName','Eurostat_Name'], keep=False)]\n",
    "\n",
    "# Count the occurrences of each CompanyName and Eurostat_Name combination in duplicates32\n",
    "company_subsector_count = duplicates32.groupby(['CompanyName', 'Eurostat_Name']).size().reset_index(name='Count')\n",
    "\n",
    "# Convert relevant columns to numeric, handling errors by setting them to NaN\n",
    "columns_to_convert = ['Capital', 'level_1_Tj', 'level_2_Tj', 'level_3_Tj', \n",
    "                      'level_1_r_Tj', 'level_2_r_Tj', 'level_3_r_Tj']\n",
    "\n",
    "df_merged3[columns_to_convert] = df_merged3[columns_to_convert].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Define the function to proportionally distribute the values in the specified columns\n",
    "def proportionally_distribute_values(group):\n",
    "    total_capital = group['Capital'].sum()  # Total Capital for the group\n",
    "    company_name = group['CompanyName'].iloc[0]  # Get the CompanyName for this group\n",
    "    subsector_name = group['Eurostat_Name'].iloc[0]  # Get the Eurostat_Name for this group\n",
    "    \n",
    "    # Get the count of this CompanyName and Eurostat_Name combination from company_subsector_count\n",
    "    match = company_subsector_count[\n",
    "        (company_subsector_count['CompanyName'] == company_name) &\n",
    "        (company_subsector_count['Eurostat_Name'] == subsector_name)\n",
    "    ]\n",
    "    \n",
    "    if not match.empty:\n",
    "        count_in_duplicates2 = match['Count'].values[0]\n",
    "    else:\n",
    "        # Handle case where the combination is not found\n",
    "        count_in_duplicates2 = 2  # Default to 2 \n",
    "    \n",
    "    # Calculate the divisor based on the count in company_subsector_count\n",
    "    divisor = count_in_duplicates2 * 0.5\n",
    "    if divisor == 0:  # To avoid division by zero\n",
    "        divisor = 1\n",
    "    \n",
    "    # List of columns to proportionally distribute\n",
    "    columns_to_distribute = ['level_1_Tj', 'level_2_Tj', 'level_3_Tj', \n",
    "                             'level_1_r_Tj', 'level_2_r_Tj', 'level_3_r_Tj']\n",
    "    \n",
    "    # Proportionally distribute the values in these columns based on 'Capital'\n",
    "    for col in columns_to_distribute:\n",
    "        original_value = group[col].iloc[0]  # Assuming the values are the same within the group\n",
    "        group = group.copy()  # Make a copy to avoid SettingWithCopyWarning\n",
    "        #group[col + '_proportional'] = (original_value * (group['Capital'] / total_capital)) / divisor\n",
    "        group[col + '_proportional'] = ((original_value * (group['Capital'] / total_capital)) / divisor).round(4)\n",
    "    return group\n",
    "\n",
    "# Apply the function to the groups, grouping by both 'CompanyName' and 'Eurostat_Name'\n",
    "df_adjusted = df_merged3.groupby(['CompanyName', 'Eurostat_Name'], group_keys=False).apply(proportionally_distribute_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff05830e-35e7-4965-8909-7f81a3c3bb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ref = df_adjusted.drop(columns=['CompanyName','Latitude','Longitude','address','registered_address'])\n",
    "df_ref = df_ref.rename(columns={'name': 'Name', 'register_identifier': 'Register_identifier','location_address': 'Address','location_long': 'Longitude','location_lat':'Latitude'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744eafaa-591f-4ba8-80cb-f305c89803a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = db.connect('mydatabase.db')\n",
    "\n",
    "# Save the DataFrame to a table in the SQLite database\n",
    "df_ref.to_sql('Heat_potential_final_subsectors_F', conn, if_exists='replace', index=False)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3fb97f-4756-4656-827c-9b99974c45d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5003665a-1af1-4fd4-a2c0-6b1ae284de0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
