{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18b476d-b61e-4378-b059-3d19f6b9a292",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 as db\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "# List of common suffixes to remove\n",
    "suffixes = ['gmbh', 'co', 'kg', 'inc', 'llc', 'ltd', 'ag', 'corporation', 'corp','deutschland','raffinerie','oel','werk','nord','sud','europa','holding','europe','se','oil','aluminium','leuna','trebsen']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f75c59-e147-4d7d-9109-d917e3b95fc0",
   "metadata": {},
   "source": [
    "## 1. Load databases and extract key words from company names for further comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88271dbe-2685-4669-8d2b-972fd032e617",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load heat production database\n",
    "# If the first row contains headers, you can skip it using the 'header' parameter:\n",
    "df_ref = pd.read_excel('points.xlsx')\n",
    "\n",
    "# Ensure all columns are converted to strings before concatenation\n",
    "df_ref['StreetNameAndNumber'] = df_ref['StreetNameAndNumber'].astype(str)\n",
    "df_ref['PostalCode'] = df_ref['PostalCode'].astype(str)\n",
    "\n",
    "# Perform the concatenation\n",
    "df_ref['Address'] = df_ref['StreetNameAndNumber'] + ',' + df_ref['PostalCode'] + ', Deutschland'\n",
    "\n",
    "df_ref = df_ref.drop(columns=['Unnamed: 0','SiteId','ID_Eurostat','ProcessInfo','NUTS1ID','NUTS3ID','geometry','StreetNameAndNumber','Country'])\n",
    "\n",
    "print(len(df_ref))\n",
    "df_ref.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcc6fcd-298f-4b83-843b-6ca7828c7879",
   "metadata": {},
   "source": [
    "##### Selecting main words from company names to search for them in the Handelsregister database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb996b43-0e05-4448-8ce1-1e57d9e7d660",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_ref\n",
    "\n",
    "# Function to clean company names\n",
    "def clean_name(name):\n",
    "    # Remove common suffixes and extra whitespace\n",
    "    pattern = r'\\b(?:' + '|'.join(suffixes) + r')\\b'\n",
    "    # Remove special characters (e.g., & . ,)\n",
    "    name = re.sub(r'[&.,+()]', '', name)\n",
    "    \n",
    "    return re.sub(pattern, '', name, flags=re.IGNORECASE).strip()\n",
    "\n",
    "# Apply the clean_name function to create a new column for comparison\n",
    "df['CleanedName'] = df['CompanyName'].apply(clean_name)\n",
    "\n",
    "print(\"length df: \", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dee402a-6f05-4aa6-a8d4-0248a4478ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load handelsregister\n",
    "conn = db.connect('latlongsdata.db')\n",
    "df_h = pd.read_sql_query('select * from Lat_Long_Table_HandelsregisterV3', conn)\n",
    "#df_h = pd.read_sql_query('select * from Lat_Long_Table_Handelsregister_Referencepaper', conn)\n",
    "conn.close()\n",
    "df_h = df_h.drop_duplicates(subset=['name','register_identifier','zip'], keep='first')\n",
    "print(len(df_h))\n",
    "#df_h = df_h.drop(columns=['level_0'])\n",
    "df_h.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd58328-11dd-46ab-8034-54de483bd785",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df_h\n",
    "\n",
    "# Function to clean company names\n",
    "def clean_name(name):\n",
    "    # Remove common suffixes and extra whitespace\n",
    "    pattern = r'\\b(?:' + '|'.join(suffixes) + r')\\b'\n",
    "    # Remove special characters (e.g., & . ,)\n",
    "    name = re.sub(r'[&.,+()]', '', name)\n",
    "    \n",
    "    return re.sub(pattern, '', name, flags=re.IGNORECASE).strip()\n",
    "\n",
    "# Apply the clean_name function to create a new column for comparison\n",
    "df2['CleanedName'] = df2['name'].apply(clean_name)\n",
    "\n",
    "print(\"length df: \", len(df2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc51114e-b144-472e-9583-42882ba169fd",
   "metadata": {},
   "source": [
    "## 2. Finding coincidences based on comparing company names' key words and then postal codes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6199765e-3c1a-42c1-8cdd-da2c44cc1e33",
   "metadata": {},
   "source": [
    "### A. Compare names and then postal codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd73e41-1382-4669-ac4f-0286b633e5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df\n",
    "\n",
    "df2 = df2\n",
    "\n",
    "# Create an empty list to store merged rows\n",
    "merged_rows = []\n",
    "\n",
    "# Iterate through each row in df1\n",
    "for i, row1 in df1.iterrows():\n",
    "    words1 = set(row1['CleanedName'].split())\n",
    "    \n",
    "    # Compare with each row in df2\n",
    "    for j, row2 in df2.iterrows():\n",
    "        words2 = set(row2['CleanedName'].split())\n",
    "        \n",
    "        # If there's at least one common word, merge the rows\n",
    "        if words1.intersection(words2):\n",
    "            merged_row = {**row1, **row2}  # Merge the two rows into one dictionary\n",
    "            merged_rows.append(merged_row)  # Add the merged row to the list\n",
    "\n",
    "# Convert the list of merged rows into a new DataFrame\n",
    "merged_df = pd.DataFrame(merged_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fb5336-7664-4435-8450-01fbae609f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep rows where postalcodes coincide\n",
    "filtered_df = merged_df[merged_df['zip'] == merged_df['PostalCode']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d81c361-269a-4947-a9df-415317cb2917",
   "metadata": {},
   "source": [
    "### B. Comparing postal codes and then names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac74f87c-903a-4024-a8b7-4d06ab93493e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#comparing postal code\n",
    "# Merge the two dataframes on the Postal code column\n",
    "filtered_df2 = pd.merge(df_h, df_ref, left_on='zip', right_on='PostalCode', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904d0438-7965-41a3-8618-c798243c3e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = filtered_df2\n",
    "# Function to clean and split text into words\n",
    "def clean_and_split(text):\n",
    "    # Remove special characters and split into words\n",
    "    words = re.sub(r'[^\\w\\s]', '', text).lower().split()\n",
    "    return [word for word in words if word not in suffixes]\n",
    "\n",
    "# Apply the function to both columns\n",
    "merged_df['name_words'] = merged_df['name'].apply(clean_and_split)\n",
    "merged_df['CompanyName_words'] = merged_df['CompanyName'].apply(clean_and_split)\n",
    "\n",
    "# Function to check if there is any common word between two lists\n",
    "def has_common_word(list1, list2):\n",
    "    return any(word in list2 for word in list1)\n",
    "\n",
    "# Filter the DataFrame\n",
    "filteredmerged_df = merged_df[merged_df.apply(lambda row: has_common_word(row['name_words'], row['CompanyName_words']), axis=1)]\n",
    "\n",
    "# Drop the helper columns if needed\n",
    "filteredmerged_df = filteredmerged_df.drop(columns=['name_words', 'CompanyName_words'])\n",
    "filteredmerged_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dc7a76-9797-47be-bd5e-1e612d2c01c7",
   "metadata": {},
   "source": [
    "## C. Now we merge both (1filtered_df and 2filteredmerged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88c399f-885a-464d-bc32-b4a90bc5618a",
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredmerged_df = filteredmerged_df.drop(columns=['CleanedName_x', 'CleanedName_y'])\n",
    "\n",
    "# Concatenate the two DataFrames\n",
    "merged_dfh2pot = pd.concat([filteredmerged_df, filtered_df])\n",
    "\n",
    "# Drop duplicate rows\n",
    "# Optionally, you can specify which columns to consider for detecting duplicates\n",
    "merged_dfh2pot.drop_duplicates(subset=['H2-Potential in TWh', 'CompanyName'], keep='first')\n",
    "merged_dfh2pot = merged_dfh2pot.drop_duplicates()\n",
    "len(merged_dfh2pot)\n",
    "finaldf = merged_dfh2pot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcc2a83-3857-4906-ad42-c50f73760697",
   "metadata": {},
   "source": [
    "## 3. Check from which documents I can get information from the XML files available in the Handelsregister"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8779b7-41fa-49fb-915f-208643f9190e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#antes de runear esto asegurarme de que todos los si docs nuevos est√°n en la carpeta de si docs from other tries \n",
    "import os\n",
    "df\n",
    "df = merged_dfh2pot\n",
    "df = df[['name','register_identifier']]\n",
    "\n",
    "df = df.rename(columns={'name': 'Name', 'register_identifier': 'Reference_number'})\n",
    "\n",
    "# Directory where the documents are stored\n",
    "doc_directory = r\"Directory_X\"\n",
    "\n",
    "# Lists to store company names and reference numbers with and without matching documents\n",
    "matching_companies = []\n",
    "matching_refnum = []\n",
    "no_matching_companies = []\n",
    "no_matching_refnum = []\n",
    "\n",
    "# Iterate through each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    company_name = row['Name']\n",
    "    reference_number = str(row['Reference_number'])\n",
    "\n",
    "    # Flag to check if a matching document was found\n",
    "    found = False\n",
    "\n",
    "    # Search term is simply the reference number\n",
    "    search_term = reference_number\n",
    "\n",
    "    for doc_name in os.listdir(doc_directory):\n",
    "        if search_term in doc_name:\n",
    "            matching_companies.append(company_name)\n",
    "            matching_refnum.append(reference_number)\n",
    "            found = True\n",
    "            break  # Exit loop if a match is found\n",
    "\n",
    "    if not found:\n",
    "        #print(f\"No document found for {company_name} with reference number {reference_number}\")\n",
    "        no_matching_companies.append(company_name)\n",
    "        no_matching_refnum.append(reference_number)\n",
    "\n",
    "print(\"Xml files found for \", len(matching_companies), \"companies\")\n",
    "print(\"Xml files not found for \", len(no_matching_companies), \"companies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851d1c29-6e42-4bf8-b6f3-fa4b54b9f364",
   "metadata": {},
   "source": [
    "## 4. Extract GRUNDKAPITAL / STAMMKAPITAL / HAFTEINLAGE values and add them to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e0fea0-9af7-4a5c-b334-38c38d949b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as et\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming matching_companies and matching_refnum are already defined\n",
    "combined_data = list(zip(matching_companies, matching_refnum))\n",
    "\n",
    "# Create DataFrame from combined data\n",
    "df_grundkapital = pd.DataFrame(combined_data, columns=['Name', 'Reference_number'])\n",
    "stammkapital = []\n",
    "grundkapital = []\n",
    "hafteinlage = []\n",
    "\n",
    "# Directory containing XML files\n",
    "download_directory = r\"Directory_X\"\n",
    "\n",
    "# Define the namespaces\n",
    "namespaces = {\n",
    "    'xjustiz': 'http://www.xjustiz.de'\n",
    "}\n",
    "\n",
    "# Iterate over each company\n",
    "for company in df_grundkapital.Name:\n",
    "    # Get the reference number of the company\n",
    "    reference_number = str(df_grundkapital[df_grundkapital['Name'] == company]['Reference_number'].iloc[0])\n",
    "    \n",
    "    xml_file_path = None\n",
    "    for xml_file in os.listdir(download_directory):\n",
    "        if xml_file.endswith(\".xml\") and reference_number in xml_file:\n",
    "            xml_file_path = os.path.join(download_directory, xml_file)\n",
    "            break  # Exit loop if a matching file is found\n",
    "\n",
    "    if xml_file_path:\n",
    "        with open(xml_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            xml_content = file.read()\n",
    "        \n",
    "        xml_tree = et.fromstring(xml_content)\n",
    "\n",
    "        # STAMMKAPITAL\n",
    "        stammkapital_value = xml_tree.findall(\".//xjustiz:fachdatenRegister/xjustiz:auswahl_zusatzangaben/xjustiz:kapitalgesellschaft/xjustiz:zusatzGmbH/xjustiz:stammkapital/xjustiz:zahl\", namespaces)\n",
    "        stammkapital.append(stammkapital_value[0].text if stammkapital_value else \"0\")\n",
    "\n",
    "        # GRUNDKAPITAL\n",
    "        grundkapital_value = xml_tree.findall(\".//xjustiz:fachdatenRegister/xjustiz:auswahl_zusatzangaben/xjustiz:kapitalgesellschaft/xjustiz:zusatzAktiengesellschaft/xjustiz:grundkapital/xjustiz:hoehe/xjustiz:zahl\", namespaces)\n",
    "        grundkapital.append(grundkapital_value[0].text if grundkapital_value else \"0\")\n",
    "\n",
    "        # HAFTEINLAGE - Extract all, sum them up, and append to the list\n",
    "        hafteinlage_elements = xml_tree.findall(\".//xjustiz:fachdatenRegister/xjustiz:auswahl_zusatzangaben/xjustiz:personengesellschaft/xjustiz:zusatzGmbH/xjustiz:datenKommanditist/xjustiz:hafteinlage/xjustiz:zahl\", namespaces)\n",
    "        if not hafteinlage_elements:\n",
    "            hafteinlage_elements = xml_tree.findall(\".//xjustiz:fachdatenRegister/xjustiz:auswahl_zusatzangaben/xjustiz:personengesellschaft/xjustiz:zusatzKG/xjustiz:datenKommanditist/xjustiz:hafteinlage/xjustiz:zahl\", namespaces)\n",
    "        \n",
    "        total_hafteinlage = sum(float(el.text) for el in hafteinlage_elements)\n",
    "        hafteinlage.append(str(total_hafteinlage) if hafteinlage_elements else \"0\")\n",
    "    else:\n",
    "        stammkapital.append(\"0\")\n",
    "        grundkapital.append(\"0\")\n",
    "        hafteinlage.append(\"0\")\n",
    "\n",
    "# Add the extracted data to the DataFrame\n",
    "df_grundkapital['Stammkapital'] = stammkapital\n",
    "df_grundkapital['Grundkapital'] = grundkapital\n",
    "df_grundkapital['Hafteinlage'] = hafteinlage\n",
    "\n",
    "# Display the final DataFrame\n",
    "print(df_grundkapital)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7a5a24-1358-4601-b2e6-e935eb110120",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grundkapitaldd = df_grundkapital.drop_duplicates()\n",
    "len(df_grundkapitaldd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbecfe8-9ac0-435a-9ca4-9d0eb50c0e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust format of dataframe\n",
    "df_grundkapital.rename(columns={'Reference_number': 'register_identifier'}, inplace=True)\n",
    "combined_df = pd.merge(df_grundkapital, finaldf, on='register_identifier', how='inner')\n",
    "combined_df = combined_df.drop(columns=['Name','CleanedName'])\n",
    "combined_df = combined_df.drop_duplicates()\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94478627-c65d-4756-807a-bc81568b5d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to SQLite database (or create it if it doesn't exist)\n",
    "conn = db.connect('databse.db')\n",
    "\n",
    "# Save the DataFrame to a table in the SQLite database\n",
    "combined_df.to_sql('Hydrogen-Capital', conn, if_exists='replace', index=False)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea2651a-792e-4b57-9099-98b9d1a60104",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(combined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49b7ca4-54bf-4117-a56c-04ba5da9227a",
   "metadata": {},
   "source": [
    "## 5. Save names for which results were not found for further searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4c3848-f520-4db7-bf28-0876cdea3780",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_ref = df_ref[~df_ref['CompanyName'].isin(combined_df['CompanyName'])]\n",
    "\n",
    "filtered_df_ref = filtered_df_ref.drop(columns=['Subsector_Name','H2-Potential in TWh'])\n",
    "filtered_df_ref = filtered_df_ref.assign(source='Heatpot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b63a952-2234-4129-ab12-c3643e6476e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = db.connect('database.db')\n",
    "\n",
    "# Save the DataFrame to a table in the SQLite database\n",
    "filtered_df_ref.to_sql('NotMatched', conn, if_exists='replace', index=False)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f3d783-5e22-4eb0-a269-5ca58f5f45e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = list(zip(no_matching_companies, no_matching_refnum))\n",
    "combined_data = pd.DataFrame(combined_data, columns=['Name', 'Reference_number'])\n",
    "\n",
    "conn = db.connect('database.db')\n",
    "\n",
    "# Save the DataFrame to a table in the SQLite database\n",
    "combined_data.to_sql('NotMatched', conn, if_exists='replace', index=False)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab5f689-a1c7-487c-a2e4-631374d36e20",
   "metadata": {},
   "source": [
    "## 6. Cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d32b1a-05b6-4815-807f-b7f8993a9aee",
   "metadata": {},
   "source": [
    "#### List of company names to delete: these have been found by manually checking the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b475cd-97e9-43a6-9fc0-ae2748f3f3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = combined_df\n",
    "\n",
    "# Define the condition for the rows to be deleted\n",
    "condition1 = (df['name'] == 'M√§rker Kalk GmbH') & (df['CompanyName'] == 'M√§rker Zementwerk GmbH')\n",
    "# Delete the rows that meet the condition\n",
    "df = df[~condition1]\n",
    "\n",
    "condition2 = (df['name'] == 'M√§rker Zementwerk GmbH') & (df['CompanyName'] == 'M√§rker Kalk GmbH')\n",
    "df = df[~condition2]\n",
    "\n",
    "condition3 = (df['name'] == 'M√§rker Zement GmbH') & (df['CompanyName'] == 'M√§rker Kalk GmbH')\n",
    "df = df[~condition3]\n",
    "\n",
    "\n",
    "df_h2b = df\n",
    "\n",
    "companies_to_delete = [\n",
    "    \"Katharina Tillmann Papier und Wellpappenfabrik e.K.\",\n",
    "    \"ArcelorMittal Duisburg GmbH\",\n",
    "    \"TRIMET ALUMINIUM SE, Niederlassung Hamburg\",\n",
    "    \"Mayr-Melnhof Gernsbach GmbH\"\n",
    "]\n",
    "\n",
    "# Filter the DataFrame to get the rows to delete\n",
    "deleted_rows = df_h2b[df_h2b['CompanyName'].isin(companies_to_delete)]\n",
    "\n",
    "# Filter the DataFrame to exclude the specified companies\n",
    "df_filtered = df_h2b[~df_h2b['CompanyName'].isin(companies_to_delete)]\n",
    "\n",
    "# Reset the index of the resulting DataFrame\n",
    "df_filtered = df_filtered.reset_index(drop=True)\n",
    "\n",
    "# Display the deleted rows\n",
    "print(\"Deleted Rows:\")\n",
    "print(deleted_rows)\n",
    "\n",
    "df_h2b = df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6f585e-a745-4a5c-b621-a0de60327e64",
   "metadata": {},
   "source": [
    "#### Merging administrative subsidiaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21ecb81-f120-48eb-8965-0eb68eddad70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_h2b\n",
    "\n",
    "# Groups to combine\n",
    "companies_merge = {\n",
    "    'ArcelorMittal Duisburg GmbH': ['ArcelorMittal Duisburg Beteiligungsgesellschaft mbH'],\n",
    "    'BASF Fuel Cell GmbH': ['BASF Fuel Cell Pensionsverwaltung GmbH'],\n",
    "    'BASF SE': ['BASF VC Beteiligungs- und Managementgesellschaft mbH', \n",
    "                'BASF Trostberger Grundbesitz GmbH', \n",
    "                'BASF Ludwigshafen Grundbesitz SE & Co. KG', \n",
    "                'BASF Lizenz GmbH', \n",
    "                'BASF Jobmarkt GmbH', \n",
    "                'BASF Beteiligungsgesellschaft mbH', \n",
    "                'BASF Battery Technology Investment GmbH & Co. KG', \n",
    "                'BASF watertechnologies GmbH & Co. KG', \n",
    "                'BASF Biorenewable Beteiligungs GmbH & Co. KG'],\n",
    "    'Panther Packaging GmbH & Co.KG': ['Beteiligungsgesellschaft Panther Packaging mbH'],\n",
    "    'Covestro AG': ['Covestro Second Real Estate GmbH'],\n",
    "    'DS Smith Paper Deutschland GmbH': ['DS Smith Transport Services GmbH'],\n",
    "    'Dow Deutschland Inc., Zweigniederlassung Stade': ['Dow Deutschland Anlagengesellschaft mbH'],\n",
    "    'Glatfelter Gernsbach GmbH': ['Glatfelter Services GmbH'],\n",
    "    'INOVYN Schkopau GmbH': ['INOVYN Sales GmbH'],  \n",
    "    'Mets√§ Tissue GmbH':['Mets√§ Tissue Immobilienverwaltungs GmbH'],\n",
    "    'Mercer Stendal GmbH': ['Mercer Stendal Logistik GmbH'],\n",
    "    'Mineraloelraffinerie Oberrhein GmbH & Co. KG': ['Mineraloelraffinerie Oberrhein Verwaltungs GmbH'],\n",
    "    'OMV Deutschland GmbH': ['OMV Deutschland Services GmbH', 'OMV Deutschland Operations GmbH & Co. KG'],\n",
    "    'Stahlwerk Annah√ºtte Max Aicher GmbH & Co. KG': ['Stahlwerk Annah√ºtte Beteiligung GmbH'],\n",
    "    'Wepa Deutschland GmbH & Co. KG': ['Wepa Deutschland Verwaltungs-GmbH']\n",
    "}\n",
    "\n",
    "# Columns to sum\n",
    "column_sum = ['Stammkapital', 'Grundkapital', 'Hafteinlage']\n",
    "\n",
    "# Ensure columns to sum are numeric\n",
    "df[column_sum] = df[column_sum].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Iterate through the company groups\n",
    "for company_principal, company_sec in companies_merge.items():\n",
    "    # Filter rows for the main and secondary companies\n",
    "    df_principal = df[df['name'] == company_principal]\n",
    "    df_sec = df[df['name'].isin(company_sec)]\n",
    "    \n",
    "    # If both principal and secondary companies exist\n",
    "    if not df_principal.empty and not df_sec.empty:\n",
    "        # Sum all values from secondary companies across the three columns\n",
    "        sum_val_sec = df_sec[column_sum].sum(skipna=True).sum()\n",
    "        \n",
    "        # Identify the non-zero column in the primary company\n",
    "        non_zero_col = df_principal[column_sum].iloc[0].idxmax()\n",
    "        \n",
    "        # Add the sum of the secondary companies to the non-zero column of the principal company\n",
    "        df.loc[df['name'] == company_principal, non_zero_col] += sum_val_sec\n",
    "\n",
    "        # Remove the rows of the secondary companies\n",
    "        df = df[~df['name'].isin(company_sec)]\n",
    "\n",
    "# Resulting DataFrame\n",
    "df_h2b = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e740b002-83e7-46bb-a44d-82a497638152",
   "metadata": {},
   "source": [
    "#### Group by the columns 'name', 'register_identifier', 'zip', and 'CompanyName' Sum the 'H2-Potential in TWh' and aggregate other columns (taking the first occurrence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c7b63a-7028-4f2f-b987-f62db89ae04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged2 = df_h2b.groupby(['name', 'register_identifier', 'zip', 'CompanyName','Subsector_Name'], as_index=False).agg({\n",
    "    'Stammkapital': 'first',\n",
    "    'Grundkapital': 'first',\n",
    "    'Hafteinlage': 'first',\n",
    "    'location_lat': 'first',\n",
    "    'location_long': 'first',\n",
    "    'location_address': 'first',\n",
    "    'registered_address': 'first',\n",
    "    'Latitude': 'first',\n",
    "    'Longitude': 'first',\n",
    "    'H2-Potential in TWh': 'sum',  # Summing the H2-Potential in TWh\n",
    "    'Address': 'first'\n",
    "})\n",
    "\n",
    "len(df_merged2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f6ffd9-9fa2-4fac-8fa6-0c6f88d28443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure columns are treated as numbers\n",
    "# distinguishing between sectors\n",
    "df = df_merged2\n",
    "df['Stammkapital'] = pd.to_numeric(df['Stammkapital'])\n",
    "df['Grundkapital'] = pd.to_numeric(df['Grundkapital'])\n",
    "df['Hafteinlage'] = pd.to_numeric(df['Hafteinlage'])\n",
    "\n",
    "# Create a new column 'Capital'\n",
    "df['Capital'] = df['Stammkapital'] + df['Grundkapital'] + df['Hafteinlage']\n",
    "\n",
    "# Format 'Capital' column to have two decimal places\n",
    "df['Capital'] = df['Capital'].apply(lambda x: \"{:.2f}\".format(x))\n",
    "df_merged3 = df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8300715-ea31-49d3-b5c5-1c7b85d2153e",
   "metadata": {},
   "source": [
    "##### Divide potential proportionally between the companies from Handelsregister associated to a company in the reference database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb6a920-dd98-49cc-98e1-b8b384bbea39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert relevant columns to numeric, handling errors by setting them to NaN\n",
    "df_merged3['Capital'] = pd.to_numeric(df_merged3['Capital'], errors='coerce')\n",
    "df_merged3['H2-Potential in TWh'] = pd.to_numeric(df_merged3['H2-Potential in TWh'], errors='coerce')\n",
    "\n",
    "\n",
    "# Count the occurrences of each CompanyName in duplicates3\n",
    "duplicates3 = df_merged3[df_merged3.duplicated(subset=['CompanyName','Subsector_Name'], keep=False)]\n",
    "# Count the occurrences of each CompanyName and Subsector_Name combination in duplicates32\n",
    "company_subsector_count = duplicates3.groupby(['CompanyName', 'Subsector_Name']).size().reset_index(name='Count')\n",
    "\n",
    "# Define the function to proportionally distribute 'H2-Potential in TWh'\n",
    "def proportionally_distribute_potential(group):\n",
    "    total_capital = group['Capital'].sum()  # Total Capital for the group\n",
    "    h2_potential = group['H2-Potential in TWh'].iloc[0]  \n",
    "    company_name = group['CompanyName'].iloc[0]  # Get the CompanyName for this group\n",
    "    subsector_name = group['Subsector_Name'].iloc[0]  # Get the Subsector_Name for this group\n",
    "    \n",
    "    # Find the count of this combination in company_subsector_count\n",
    "    match = company_subsector_count[\n",
    "        (company_subsector_count['CompanyName'] == company_name) &\n",
    "        (company_subsector_count['Subsector_Name'] == subsector_name)\n",
    "    ]\n",
    "    \n",
    "    if not match.empty:\n",
    "        count_in_duplicates2 = match['Count'].values[0]\n",
    "    else:\n",
    "        count_in_duplicates2 = 2  # Default to 1 if combination is not found\n",
    "    \n",
    "    # Calculate the divisor based on the count in company_subsector_count\n",
    "    divisor = count_in_duplicates2 * 0.5\n",
    "    if divisor == 0:  # To avoid division by zero\n",
    "        divisor = 1\n",
    "    \n",
    "    # Proportionally distribute the 'H2-Potential in TWh' based on 'Capital' and divide by the divisor\n",
    "    distributed_potential = (h2_potential * (group['Capital'] / total_capital)) / divisor\n",
    "    \n",
    "    # Create a new column to avoid SettingWithCopyWarning\n",
    "    group = group.copy()\n",
    "    #group['Distributed H2-Potential in TWh'] = distributed_potential\n",
    "    group['Distributed H2-Potential in TWh'] = distributed_potential.round(4)\n",
    "\n",
    "    return group\n",
    "\n",
    "# Apply the function, grouping by both 'CompanyName' and 'Subsector_Name'\n",
    "df_divided_potential = df_merged3.groupby(['CompanyName', 'Subsector_Name'], group_keys=False).apply(proportionally_distribute_potential)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb43b95-9d02-4720-bbda-f2b3341680bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ref = df_divided_potential.drop(columns=['CompanyName','Latitude','Longitude','Address','registered_address'])\n",
    "df_ref = df_ref.rename(columns={'name': 'Name', 'register_identifier': 'Register_identifier','location_address': 'Address','location_long': 'Longitude','location_lat':'Latitude','zip': 'Postalcode'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5f6ba9-963f-4489-9f62-6bff81461ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = db.connect('mydatabase.db')\n",
    "\n",
    "df_ref.to_sql('H2_final_subsectors_F', conn, if_exists='replace', index=False)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
